{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surrounded-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Input, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noticed-tragedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        굳\n",
       "1        G\n",
       "2        뭐\n",
       "3        지\n",
       "4        3\n",
       "        ..\n",
       "49995    오\n",
       "49996    의\n",
       "49997    그\n",
       "49998    절\n",
       "49999    마\n",
       "Name: document, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "import re\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "test_data['document'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "widespread-bradley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "little-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab() # 토크나이저 생성을 위한 Mecab() 활용\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'] # 불용어 정의\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    # drop_duplicates: 중복 데이터 제거 메소드이며 subset은 처리할 열, 끝에 inplace는 중복 데이터 제거 후 원래 변수에 재할당하는 것\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    # 결측치 제거이며 how='any'는 무엇이든 결측치가 있다면 다 제거하는 것\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True) \n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        # Mecab().morphs 메소드는 한글 형태소 분석 방법 중 하나이다.\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    # concatenate을 활용해 array를 합쳐주고 tolist로 리스트로 변환\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    # Counter()를 활용해 words리스트 안의 인자별 개수를 세서 [('text', 반복횟수)]\n",
    "    counter = Counter(words)\n",
    "    # 그리고 counter.most_common를 활용해 가장 많은 개수를 가진 인자부터 재배열한다.\n",
    "    # counter.most_common(k)에서 가장 개수가 많은 k개의 데이터를 얻을 수 있다.\n",
    "    counter = counter.most_common(10000-4)\n",
    "\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "capital-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "union-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metallic-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n",
      "pad_sequences maxlen :  41\n",
      "전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lyric-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "serial-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "juvenile-possession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136182, 41)\n",
      "(136182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = X_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_X_train = X_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_X_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-intro",
   "metadata": {},
   "source": [
    "# 모델1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "heated-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "appreciated-monkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "266/266 [==============================] - 6s 12ms/step - loss: 0.6060 - accuracy: 0.7088 - val_loss: 0.3633 - val_accuracy: 0.8484\n",
      "Epoch 2/5\n",
      "266/266 [==============================] - 3s 10ms/step - loss: 0.3467 - accuracy: 0.8556 - val_loss: 0.3467 - val_accuracy: 0.8475\n",
      "Epoch 3/5\n",
      "266/266 [==============================] - 3s 10ms/step - loss: 0.3175 - accuracy: 0.8693 - val_loss: 0.3410 - val_accuracy: 0.8515\n",
      "Epoch 4/5\n",
      "266/266 [==============================] - 3s 10ms/step - loss: 0.3080 - accuracy: 0.8715 - val_loss: 0.3433 - val_accuracy: 0.8506\n",
      "Epoch 5/5\n",
      "266/266 [==============================] - 3s 10ms/step - loss: 0.3000 - accuracy: 0.8750 - val_loss: 0.3439 - val_accuracy: 0.8526\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abandoned-manner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 5s - loss: 0.3582 - accuracy: 0.8465\n",
      "[0.35823580622673035, 0.8465121984481812]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-alberta",
   "metadata": {},
   "source": [
    "# 모델2 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unable-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10000개의 단어)\n",
    "word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "understanding-frank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "266/266 [==============================] - 11s 26ms/step - loss: 0.5874 - accuracy: 0.6753 - val_loss: 0.3493 - val_accuracy: 0.8424\n",
      "Epoch 2/5\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.3292 - accuracy: 0.8609 - val_loss: 0.3357 - val_accuracy: 0.8519\n",
      "Epoch 3/5\n",
      "266/266 [==============================] - 2s 7ms/step - loss: 0.2973 - accuracy: 0.8760 - val_loss: 0.3349 - val_accuracy: 0.8538\n",
      "Epoch 4/5\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2709 - accuracy: 0.8893 - val_loss: 0.3350 - val_accuracy: 0.8546\n",
      "Epoch 5/5\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 0.2418 - accuracy: 0.9059 - val_loss: 0.3404 - val_accuracy: 0.8539\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "economic-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 5s - loss: 0.3548 - accuracy: 0.8506\n",
      "[0.3548415005207062, 0.8506418466567993]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-paris",
   "metadata": {},
   "source": [
    "# 모델3 GlobalMaxPolling1D() 레이어 하나만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "environmental-advantage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10000개의 단어)\n",
    "word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "varied-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6632 - accuracy: 0.6385 - val_loss: 0.4672 - val_accuracy: 0.8201\n",
      "Epoch 2/5\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.4255 - accuracy: 0.8314 - val_loss: 0.3672 - val_accuracy: 0.8382\n",
      "Epoch 3/5\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.3415 - accuracy: 0.8596 - val_loss: 0.3483 - val_accuracy: 0.8449\n",
      "Epoch 4/5\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.3081 - accuracy: 0.8728 - val_loss: 0.3457 - val_accuracy: 0.8490\n",
      "Epoch 5/5\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.2863 - accuracy: 0.8838 - val_loss: 0.3473 - val_accuracy: 0.8502\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "native-empire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.3594 - accuracy: 0.8458\n",
      "[0.359380841255188, 0.8458002209663391]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-plaintiff",
   "metadata": {},
   "source": [
    "# 한국어 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "equal-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/data/ko.bin'\n",
    "word2vec = gensim.models.Word2Vec.load(word2vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-terrace",
   "metadata": {},
   "source": [
    "# 유사단어 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "secure-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('감명', 0.7177015542984009),\n",
       " ('감격', 0.6908231973648071),\n",
       " ('실망', 0.6267645359039307),\n",
       " ('감화', 0.6191877126693726),\n",
       " ('감탄', 0.6140128374099731),\n",
       " ('칭찬', 0.6059398055076599),\n",
       " ('존경', 0.6032299995422363),\n",
       " ('자극', 0.594598650932312),\n",
       " ('감복', 0.5902734994888306),\n",
       " ('호응', 0.5850393772125244)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"감동\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "seventh-accuracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "satisfied-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 41, 200)           2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 35, 16)            22416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 16)             1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,024,369\n",
      "Trainable params: 2,024,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "liquid-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "mysterious-proceeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4256/4256 [==============================] - 68s 16ms/step - loss: 0.3198 - accuracy: 0.8638 - val_loss: 0.3334 - val_accuracy: 0.8526\n",
      "Epoch 2/5\n",
      "4256/4256 [==============================] - 62s 15ms/step - loss: 0.2740 - accuracy: 0.8852 - val_loss: 0.3251 - val_accuracy: 0.8591\n",
      "Epoch 3/5\n",
      "4256/4256 [==============================] - 64s 15ms/step - loss: 0.2424 - accuracy: 0.9022 - val_loss: 0.3378 - val_accuracy: 0.8579\n",
      "Epoch 4/5\n",
      "4256/4256 [==============================] - 64s 15ms/step - loss: 0.2121 - accuracy: 0.9162 - val_loss: 0.3502 - val_accuracy: 0.8603\n",
      "Epoch 5/5\n",
      "4256/4256 [==============================] - 63s 15ms/step - loss: 0.1784 - accuracy: 0.9316 - val_loss: 0.3699 - val_accuracy: 0.8556\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_X_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[es],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-proof",
   "metadata": {},
   "source": [
    "# 정확도를 85% 이상 달성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "historical-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 4s - loss: 0.3880 - accuracy: 0.8506\n",
      "[0.3879564106464386, 0.8506418466567993]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-guess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-daily",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
